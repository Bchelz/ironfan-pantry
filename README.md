= DESCRIPTION:

Cookbook to install flume on a cluster.

= REQUIREMENTS:

This recipe relies on cluster_discovery_services to determine which nodes 
across the cluster act as flume masters, and which nodes provide zookeeper
servers.

= ATTRIBUTES:

node[:flume][:cluster_name] - The name of the cluster to participate
with (masters and zookeepers...) defaults to node[:cluster_name].

node[:flume][:data_dir] - Directory flume should use to store local 
temporary files.

node[:flume][:master][:external_zookeeper] - false to use flume's
zookeeper. True to attach to an external zookeeprs.

node[:flume][:master][:zookeeper_port] - port to talk to zookeeper on
(for external zookeeper) - defaults to 2181

node[:flume][:classes] - list of classes to include as plugins 

node[:flume][:classpath] - list of directories and jars to add to the
FLUME_CLASSPATH.

noe[:flume][:java_opts] - list of command line parameters to add to the 
jvm.

If you have a particular plugin to configure, you can also configure
the classpath and the classes to include in the configuration file 
with attributes in the following forms:

node[:flume][:plugin][:plugin_name][:classes]
node[:flume][:plugin][:plugin_name][:classpath]
node[:flume][:plugin][:plugin_name][:java_opts]


Set the following two attributes to allow writing to s3 buckets:

node[:flume][:aws_access_key]
node[:flume][:aws_secret_key]


node[:flume][:collector][:output_format]
Controls what format the node writes logs (using collectorSink):
 * avro - Avro Native file format. Default currently is uncompressed.
 * avrodata - Binary encoded data written in the avro binary format.
 * avrojson - JSON encoded data generated by avro.
 * default - a debugging format.
 * json - JSON encoded data.
 * log4j - a log4j pattern similar to that used by CDH output pattern.
 * raw - Event body only. This is most similar to copying a file but does not preserve any uniqifying metadata like host/timestamp/nanos.
 * syslog - a syslog like text output format.
 
node[:flume][:collector][:codec]

Controls what kind of compression the collector will use when writing a file.
whether or not collected logs are gzipped before writing
them to their final resting place (using collectorSink)
 * GZipCodec
 * BZip2Codec

= USAGE:

Use flume::master to set up a master node. Use flume::node to set up a
physical node. Currently only one physical node per machines. 

Configure logical nodes with the logical_node resource - see the test_flow.rb 
recipe for an example. This is still somewhat experimental, and some features
will not work as well as they should until chef version 0.9.14 and others until
the next release of flume.

Coming soon flume::xxx_plugin.

