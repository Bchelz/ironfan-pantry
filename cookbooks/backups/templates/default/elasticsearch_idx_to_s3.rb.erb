#!/usr/bin/env ruby

# Simple script to dump elasticsearch indexes as raw JSON 

require 'tire'
require 'zlib'
require 'socket'
require 'pathname'
require 'optparse'
require 'multi_json'

class ES_Backup
  def initialize(index, output, host, port=9200)
    @index = index
    @dir = output
    path = Pathname.new("%s/%s.gz" %[output, index]).cleanpath.to_s
    @output = Zlib::GzipWriter.open(path)
    Tire::Configuration.url("http://%s:%s" %[host, port]) if !host.nil?
  end

  def mapping()
    index = Tire::Index.new(@index)
    m_out = File.open("#{@dir}/#{@index}.mappings", "w")
    m_out.write(index.mapping.to_json)
    m_out.close()
  end

  def scroll(size)
    options = {:size => size}
    index = Tire::Search::Scan.new(@index, options)
    index.each_document do |document|
      begin
        out = document.to_hash.except(:type, :_index, :_explanation, :_score, :_version, :highlight, :sort).to_json
        @output.write(("%s\n" %[out]))
      rescue NoMethodError
        next # For whatever reason some Hashes fail to turn into JSON!
      end
    end
    @output.close()
  end
end

tday = Time.now().strftime("%Y%m%d")
indexes = <%= @node[:backups][:elasticsearch][:indexes] %>
output = "<%= @node[:backups][:location] %>/elasticsearch-%s" %[tday]
FileUtils.mkdir_p(output) if !File.directory?(output)
host = "<%= @elasticsearch_host %>"
scroll_sz = <%= @node[:backups][:elasticsearch][:scroll] %>

indexes.each do |i|
  puts "Backing up #{i}"
  backup = ES_Backup.new(i, output, host)
  backup.mapping()
  backup.scroll(scroll_sz)
end
  
